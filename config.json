{
  "embedding": {
    "hidden_size": 128,
    "synset_vocab_size": 86572,
    "lemma_vocab_size": 148700,
    "pos_types": 6,
    "tot_sense": 60,
    "layer_norm_eps":1e-12,
    "hidden_dropout_prob":0.1
  },
  "sage": {
    "hidden_channels": 64,
    "num_layers": 2
  },
  "dgn": {
    "embedding_size": 128,
    "type": "gcn",
    "activation": "relu",
    "hidden_sizes_list": []
  },
  "transformers": {
    "pred_type": "",
    "node_embd_type": "",
    "num_layers": "",
    "layer_dim_list": "",
    "act": "",
    "dropout": ""
  }
}